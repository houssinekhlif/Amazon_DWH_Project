import pandas as pd

def clean_data(file_path):
    # Load the data
    data = pd.read_csv(file_path)

    # Drop duplicate rows
    data = data.drop_duplicates()

    # Handle missing values (e.g., fill with median for numeric columns)
    for column in data.columns:
        if data[column].dtype in ['int64', 'float64']:
            data[column] = data[column].fillna(data[column].median())
        else:
            data[column] = data[column].fillna('Unknown')

    # Strip whitespace from column names
    data.columns = data.columns.str.strip()

    # Rename columns for consistency (optional: customize as needed)
    data.rename(columns=lambda x: x.strip().replace(" ", "_"), inplace=True)

    # Ensure numeric columns are correctly typed
    for column in data.select_dtypes(include=['object']).columns:
        try:
            data[column] = pd.to_numeric(data[column])
        except ValueError:
            pass

    # Return the cleaned DataFrame
    return data

# File paths
file_path_geographical = 'C:/Users/21650/Downloads/New_script/Geographical_Data.csv'
file_path_demographic = 'C:/Users/21650/Downloads/New_script/Demographic_Data.csv'

# Clean the datasets
cleaned_geographical_data = clean_data(file_path_geographical)
cleaned_demographic_data = clean_data(file_path_demographic)

# Save cleaned data to new files
cleaned_geographical_data.to_csv('C:/Users/21650/Downloads/New_script_output/Cleaned_Geographical_Data.csv', index=False)
cleaned_demographic_data.to_csv('C:/Users/21650/Downloads/New_script_output/Cleaned_Demographic_Data.csv', index=False)

print("Datasets have been cleaned and saved.")

# Star schema creation
# Load datasets
sales_data = pd.read_csv("C:/Users/21650/Downloads/New_script/Sales.csv")
inventory_data = pd.read_csv("C:/Users/21650/Downloads/New_script/Inventory.csv")
clients_data = pd.read_csv("C:/Users/21650/Downloads/New_script/Clients.csv")
stores_data = pd.read_csv("C:/Users/21650/Downloads/New_script/Stores.csv")
cleaned_geographical_data = pd.read_csv("C:/Users/21650/Downloads/New_script_output/Cleaned_Geographical_Data.csv")
cleaned_demographic_data = pd.read_csv("C:/Users/21650/Downloads/New_script_output/Cleaned_Demographic_Data.csv")

# Remove duplicate Customer_ID rows in clients_data
clients_data = clients_data.drop_duplicates(subset=['Customer_ID'])

# Create Date Dimension
sales_data['Date'] = pd.to_datetime(sales_data['Date'])
date_dimension = sales_data[['Date']].drop_duplicates()
date_dimension['Year'] = date_dimension['Date'].dt.year
date_dimension['Month'] = date_dimension['Date'].dt.month
date_dimension['Day'] = date_dimension['Date'].dt.day
date_dimension['Weekday'] = date_dimension['Date'].dt.day_name()

# Create Product Dimension
product_dimension = inventory_data[['Product_ID', 'Category', 'Title', 'Price', 'Material', 'Cost']].drop_duplicates()

# Create Customer Dimension
customer_dimension = clients_data[['Customer_ID', 'Name', 'Gender', 'Age', 'State', 'Income_Category', 'Marital_Status']].drop_duplicates()

# Create Store Dimension
store_dimension = stores_data[['StoreID', 'StoreName', 'Location', 'OpeningDate', 'TotalEmployees', 'AnnualRevenue', 'State']].drop_duplicates()

# Create Geography Dimension
geography_dimension = cleaned_geographical_data[['Region', 'Latitude', 'Longitude', 'Current_Store_Count', 'Potential_New_Stores', 'Market_Size']].drop_duplicates()

# Create Demographics Dimension
demographics_dimension = cleaned_demographic_data[['Region', 'Population', 'Median_Income', 'Unemployment_Rate', 'Competitor_Count', 'Growth_Rate']].drop_duplicates()

# Create Sales Fact Table
sales_fact = sales_data[['Product_ID', 'Customer_ID', 'StoreID', 'Date', 'Price', 'Revenue', 'Units_Sold', 'Region']].drop_duplicates()

# Rename 'Region' column to 'Region-Geo' and add 'Region_Demo'
sales_fact.rename(columns={'Region': 'Region-Geo'}, inplace=True)
sales_fact['Region_Demo'] = sales_fact['Region-Geo']

# Save all dimensions and the fact table as CSV files
output_paths = {
    "Date Dimension": "C:/Users/21650/Downloads/New_script_output/Date_Dimension.csv",
    "Product Dimension": "C:/Users/21650/Downloads/New_script_output/Product_Dimension.csv",
    "Customer Dimension": "C:/Users/21650/Downloads/New_script_output/Customer_Dimension.csv",
    "Store Dimension": "C:/Users/21650/Downloads/New_script_output/Store_Dimension.csv",
    "Geography Dimension": "C:/Users/21650/Downloads/New_script_output/Geography_Dimension.csv",
    "Demographics Dimension": "C:/Users/21650/Downloads/New_script_output/Demographics_Dimension.csv",
    "Sales Fact Table": "C:/Users/21650/Downloads/New_script_output/Sales_Fact_Table.csv"
}

date_dimension.to_csv(output_paths["Date Dimension"], index=False)
product_dimension.to_csv(output_paths["Product Dimension"], index=False)
customer_dimension.to_csv(output_paths["Customer Dimension"], index=False)
store_dimension.to_csv(output_paths["Store Dimension"], index=False)
geography_dimension.to_csv(output_paths["Geography Dimension"], index=False)
demographics_dimension.to_csv(output_paths["Demographics Dimension"], index=False)
sales_fact.to_csv(output_paths["Sales Fact Table"], index=False)

print("Star schema created and saved:")
for name, path in output_paths.items():
    print(f"- {name}: {path}")

# Save each table as an SQL file
output_paths_sql = {
    "Date_Dimension.sql": date_dimension,
    "Product_Dimension.sql": product_dimension,
    "Customer_Dimension.sql": customer_dimension,
    "Store_Dimension.sql": store_dimension,
    "Geography_Dimension.sql": geography_dimension,
    "Demographics_Dimension.sql": demographics_dimension,
    "Sales_Fact_Table.sql": sales_fact
}

for filename, dataframe in output_paths_sql.items():
    table_name = filename.split('.')[0]
    sql_file_path = f"C:/Users/21650/Downloads/New_script/{filename}"

    with open(sql_file_path, 'w', encoding='utf-8') as sql_file:
        # Write the CREATE TABLE statement
        columns = ', '.join([f'[{col}] TEXT' for col in dataframe.columns])  # Simplified as TEXT
        sql_file.write(f"CREATE TABLE {table_name} ({columns});\n")

        # Write the INSERT INTO statements
        for _, row in dataframe.iterrows():
            values = []
            for value in row.values:
                if isinstance(value, str):
                    value = value.replace("'", "''")  # Escape single quotes for SQL
                    values.append(f"'{value}'")  # Wrap strings in single quotes
                elif pd.isna(value):
                    values.append("NULL")  # Handle missing values
                else:
                    values.append(str(value))  # Convert other values to string

            values_str = ', '.join(values)
            sql_file.write(f"INSERT INTO {table_name} VALUES ({values_str});\n")


